{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QfDwaWNTZCZ"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This notebook uses perch-hoplite to compute and save embeddings for set of audio files using a pre-trained model. This is the first step in the agile modeling process. If the data you wish to search and classify is already embedded with a pre-trained model into a perch-hoplite database, then proceed to the step 2 colab notebook ([2_agile_modeling_v2.ipynb](https://github.com/google-research/perch-hoplite/blob/main/perch_hoplite/agile/2_agile_modeling_v2.ipynb))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Db84ySxSJYA"
      },
      "source": [
        "## [Optional] perch-hoplite installation for hosted runtimes\n",
        "\n",
        "If you have not already installed perch-hoplite (particularly if you are using a hosted Colab runtime), make sure to install perch-hoplite from the Github source to ensure the most recent version is installed. After installation, you will need to restart your runtime before running anything else. Go to the top menu, select \"Runtime\" then \"Restart Session\".\n",
        "\n",
        "**If you want to use the Perch V2 model, you must be on a GPU runtime and additionally install TensorFlow version 2.20.rc0 (experimental).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XS5FgXBufrVt"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'perchhoplite (Python 3.12.12)' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n perchhoplite ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "#@title If you plan to use Perch V2, you must install this version (or later) of TensorFlow and cuda\n",
        "!pip install tensorflow[and-cuda]~=2.20.0rc0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTtVnkC-6_i7"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'audio_io'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# from agile import colab_utils\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01membed\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_info\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m brutalism\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Valentin_REVO/agile_modeling_freshwater/perch-hoplite/perch_hoplite/agile/embed.py:27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mml_collections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_dict\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maudio_io\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_info\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interface \u001b[38;5;28;01mas\u001b[39;00m hoplite_interface\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'audio_io'"
          ]
        }
      ],
      "source": [
        "# @title Imports\n",
        "# from etils import epath\n",
        "from IPython.display import display\n",
        "# import ipywidgets as widgets\n",
        "import numpy as np\n",
        "# from agile import colab_utils\n",
        "from agile import embed\n",
        "from agile import source_info\n",
        "from db import brutalism\n",
        "from db import interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T4vILrO80iP"
      },
      "source": [
        "# Embed the audio data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6zdGxl68vft"
      },
      "outputs": [],
      "source": [
        "# @title Configuration { vertical-output: true }\n",
        "\n",
        "# @markdown Configure the raw dataset and output location(s).  The format is a mapping from\n",
        "# @markdown a dataset_name to a (base_path, fileglob) pair.  Note that the file\n",
        "# @markdown globs are case sensitive.  The dataset name can be anything you want.\n",
        "#\n",
        "# @markdown This structure allows you to move your data around without having to\n",
        "# @markdown re-embed the dataset.  The generated embedding database will be\n",
        "# @markdown placed in the base path. This allows you to simply swap out\n",
        "# @markdown the base path here if you ever move your dataset.\n",
        "\n",
        "# @markdown By default we only process one dataset at a time.  Re-run this entire notebook\n",
        "# @markdown once per dataset.\n",
        "\n",
        "# @markdown For example, we might set dataset_base_path to '/home/me/myproject',\n",
        "# @markdown and use the glob '\\*/\\*.wav' if all of the audio files have filepaths\n",
        "# @markdown like '/home/me/myproject/site_XYZ/audio_ABC.wav' (e.g. audio files are contained in subfolders of the base directory).\n",
        "\n",
        "# @markdown 1. Create a unique name for the database that will store the embeddings for the target data.\n",
        "dataset_name = ''  # @param {type:'string'}\n",
        "# @markdown 2. Input the filepath for the folder that is containing the input audio files.\n",
        "dataset_base_path = ''  #@param {type:'string'}\n",
        "# @markdown 3. Input the file pattern for the audio files within that folder that you want to embed. Some examples for how to input:\n",
        "# @markdown - All files in the base directory of a specific type (not subdirectories): e.g. `*.wav` (or `*.flac` etc) will generate embeddings for all .wav files (or whichever format) in the dataset_base_path\n",
        "# @markdown - All files in one level of subdirectories within the base directory: `*/*.flac` will generate embeddings for all .flac files\n",
        "# @markdown - Single file: `myfile.wav` will only embed the audio from that specific file.\n",
        "dataset_fileglob = '*.wav'  # @param {type:'string'}\n",
        "\n",
        "# @markdown 4. [Optional] If saving the embeddings database to a new directory, specify here.\n",
        "# @markdown Otherwise, leave blank - by default the embeddings database output will be saved within\n",
        "# @markdown dataset_base_path where the audio is located. You do not need to specify db_path unless you want to maintain multiple\n",
        "# @markdown distinct embedding databases, or if you would like to save the output\n",
        "# @markdown in a different folder. If your input audio data is accessed\n",
        "# @markdown from a public URL, we recommend specifying a separate output directory here.\n",
        "db_path = ''  # @param {type:'string'}\n",
        "if not db_path or db_path == 'None':\n",
        "  db_path = None\n",
        "\n",
        "# @markdown 5. Choose a supported model to generate embeddings: `perch_v2` is the latest Perch model and was trained\n",
        "# @markdown on multiple taxa include birds, mammals, insects and amphibians; `perch_v2` has also demonstrated high performance for marine audio transfer learning tasks. **NOTE: `perch_v2` only works with GPU runtimes - see above instructions.**\n",
        "# @markdown `perch_8` is the last updated version of Perch V1 trained only on birds, and `birdnet_v2.3` is also a common option\n",
        "# @markdown for birds. Other choices include `surfperch` for coral reefs or\n",
        "# @markdown `multispecies_whale` for marine mammals.\n",
        "model_choice = 'perch_v2'  #@param['perch_v2','perch_8', 'humpback', 'multispecies_whale', 'surfperch', 'birdnet_V2.3']\n",
        "\n",
        "# @markdown 6. [Optional] Shard the audio for embeddings. File sharding automatically splits audio files into smaller chunks\n",
        "# @markdown for creating embeddings. This limits both system and GPU memory usage,\n",
        "# @markdown especially useful when working with long files (>1 hour).\n",
        "use_file_sharding = True  # @param {type:'boolean'}\n",
        "# @markdown If you want to change the length in seconds for the shards, specify here.\n",
        "shard_length_in_seconds = 60  # @param {type:'number'}\n",
        "\n",
        "audio_glob = source_info.AudioSourceConfig(\n",
        "    dataset_name=dataset_name,\n",
        "    base_path=dataset_base_path,\n",
        "    file_glob=dataset_fileglob,\n",
        "    min_audio_len_s=1.0,\n",
        "    target_sample_rate_hz=-2,\n",
        "    shard_len_s=float(shard_length_in_seconds) if use_file_sharding else None,\n",
        ")\n",
        "\n",
        "configs = colab_utils.load_configs(\n",
        "    source_info.AudioSources((audio_glob,)),\n",
        "    db_path,\n",
        "    model_config_key=model_choice,\n",
        "    db_key='sqlite_usearch',\n",
        ")\n",
        "configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN9Uyy1yqAWS"
      },
      "outputs": [],
      "source": [
        "#@title Initialize the hoplite database (DB) { vertical-output: true }\n",
        "global db\n",
        "db = configs.db_config.load_db()\n",
        "num_embeddings = db.count_embeddings()\n",
        "\n",
        "print('Initialized DB located at ', configs.db_config.db_config.db_path)\n",
        "\n",
        "def drop_and_reload_db(_) -> interface.HopliteDBInterface:\n",
        "  db_path = epath.Path(configs.db_config.db_config.db_path)\n",
        "  for fp in db_path.glob('hoplite.sqlite*'):\n",
        "    fp.unlink()\n",
        "  (db_path / 'usearch.index').unlink()\n",
        "  print('\\n Deleted previous db at: ', configs.db_config.db_config.db_path)\n",
        "  db = configs.db_config.load_db()\n",
        "\n",
        "#@markdown If `drop_existing_db` set to True, when the database already exists and contains embeddings,\n",
        "#@markdown then those existing embeddings will be erased. You will be prompted to confirm you wish to delete those existing\n",
        "#@markdown embeddings. If you want to keep existing embeddings in the database, then set to False, which will append the new\n",
        "#@markdown embeddings to the database.\n",
        "drop_existing_db = False  #@param {type:'boolean'}\n",
        "\n",
        "if num_embeddings > 0 and drop_existing_db:\n",
        "  print('Existing DB contains datasets: ', db.get_dataset_names())\n",
        "  print('num embeddings: ', num_embeddings)\n",
        "  print('\\n\\nClick the button below to confirm you really want to drop the database at ')\n",
        "  print(f'{configs.db_config.db_config.db_path}\\n')\n",
        "  print(f'This will permanently delete all {num_embeddings} embeddings from the existing database.\\n')\n",
        "  print('If you do NOT want to delete this data, set `drop_existing_db` above to `False` and re-run this cell.\\n')\n",
        "\n",
        "  button = widgets.Button(description='Delete database?')\n",
        "  button.on_click(drop_and_reload_db)\n",
        "  display(button)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnGWbhc0LhiU"
      },
      "outputs": [],
      "source": [
        "#@title Run the embedding { vertical-output: true }\n",
        "\n",
        "print(f'Embedding dataset: {audio_glob.dataset_name}')\n",
        "\n",
        "worker = embed.EmbedWorker(\n",
        "    audio_sources=configs.audio_sources_config,\n",
        "    db=db,\n",
        "    model_config=configs.model_config)\n",
        "\n",
        "worker.process_all(target_dataset_name=audio_glob.dataset_name)\n",
        "\n",
        "print('\\n\\nEmbedding complete, total embeddings: ', db.count_embeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvVuFw-somHe"
      },
      "outputs": [],
      "source": [
        "#@title Per dataset statistics { vertical-output: true }\n",
        "\n",
        "for dataset in db.get_dataset_names():\n",
        "  print(f'\\nDataset \\'{dataset}\\':')\n",
        "  print('\\tnum embeddings: ', db.get_embeddings_by_source(dataset, source_id=None).shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ihBNRbwuuwal"
      },
      "outputs": [],
      "source": [
        "#@title Show example embedding search\n",
        "#@markdown As an example (and to show that the embedding process worked), this\n",
        "#@markdown selects a single embedding from the database and outputs the embedding ids of the\n",
        "#@markdown top-K (k = 128) nearest neighbors in the database.\n",
        "\n",
        "q = db.get_embedding(db.get_one_embedding_id())\n",
        "%time results, scores = brutalism.brute_search(worker.db, query_embedding=q, search_list_size=128, score_fn=np.dot)\n",
        "print([int(r.embedding_id) for r in results])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//gdm/sustainability/perch:perch_notebook",
        "kind": "private"
      },
      "name": "v2_1_embed_unlabeled_audio.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1ePT3-fDB3kA3_T7trthFtu8xTJQWQBoQ",
          "timestamp": 1723499538314
        }
      ]
    },
    "kernelspec": {
      "display_name": "perchhoplite",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
